{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "This notebook demonstrates how `homonim` can help improve classifier accuracy.  It works with images and ground truth taken from a [vegetation mapping study](https://www.researchgate.net/publication/329137175_Regional_mapping_of_spekboom_canopy_cover_using_very_high_resolution_aerial_imagery) that sought to identify Spekboom in the Little Karoo, South Africa.  Aerial imagery with 50 cm spatial resolution and 4 spectral bands (red, green, blue and near-infrared) was obtained from the  [NGI](https://ngi.dalrrd.gov.za/index.php/what-we-do/aerial-photography-and-imagery).  Ground truth data consists of ± 160 polygons with labels for 3 classes: \n",
    "\n",
    "- Spekboom: a species of succulent shrub.\n",
    "- Tree: any other tree. \n",
    "- Background: other vegetation, bare ground etc.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Install dependencies\n",
    "\n",
    "`geedim`, `geopandas`, `gdal`, `matplotlib`, `sklearn` and `scipy` are required to run the notebook.  You can uncomment the cell below to install them, if they aren't installed already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# if 'conda' in sys.prefix:\n",
    "#     # install into the conda environment the notebook is being run from\n",
    "#     !conda install --yes --prefix {sys.prefix} -c conda-forge geedim geopandas gdal matplotlib sklearn scipy\n",
    "# else:\n",
    "#     # install into the python environment the notebook is being run from\n",
    "#     !{sys.executable} -m pip install geedim geopandas gdal matplotlib sklearn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used by more than one cell\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import geedim as gd\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise source and reference paths\n",
    "from glob import glob\n",
    "src_root = Path(\n",
    "    'V:/Data/HomonimEgs/NGI_3321BD-3322AC_2010_LittleKaroo/Source/'\n",
    "    'GroundTruthSubset'\n",
    ")\n",
    "src_mosaic_path = src_root.joinpath(\n",
    "    'NGI_3321BD-3322AC_2010_LittleKaroo_GroundTruthSubset_Source.vrt'\n",
    ")\n",
    "src_paths = [\n",
    "    Path(src_path) \n",
    "    for src_path in glob(str(src_root.joinpath('*_RGBN_CMP.tif')))\n",
    "]\n",
    "\n",
    "ref_root = Path(\n",
    "    'V:/Data/HomonimEgs/NGI_3321BD-3322AC_2010_LittleKaroo/Reference/'\n",
    "    'GroundTruthSubset'\n",
    ")\n",
    "l7_ref_path = ref_root.joinpath('l7_comp_ref.tif')\n",
    "modis_ref_path = ref_root.joinpath('modis_nbar_ref.tif')\n",
    "\n",
    "# create a search region that covers the source image mosaic\n",
    "region = gd.utils.get_bounds(src_mosaic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and download reference images\n",
    "\n",
    "Use [`geedim`](https://github.com/dugalh/geedim) to search for and download Landsat-7 and MODIS NBAR reference images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image property descriptions:\n",
      "\n",
      "ABBREV     NAME                  DESCRIPTION\n",
      "---------  --------------------  ---------------------------------------\n",
      "ID         system:id             Earth Engine image id\n",
      "DATE       system:time_start     Image capture date/time (UTC)\n",
      "FILL       FILL_PORTION          Portion of valid pixels (%)\n",
      "CLOUDLESS  CLOUDLESS_PORTION     Portion of cloud/shadow free pixels (%)\n",
      "GRMSE      GEOMETRIC_RMSE_MODEL  Orthorectification RMSE (m)\n",
      "SAA        SUN_AZIMUTH           Solar azimuth angle (deg)\n",
      "SEA        SUN_ELEVATION         Solar elevation angle (deg)\n",
      "\n",
      "Search Results:\n",
      "\n",
      "ID                                          DATE              FILL CLOUDLESS GRMSE   SAA   SEA\n",
      "------------------------------------------- ---------------- ----- --------- ----- ----- -----\n",
      "LANDSAT/LE07/C02/T1_L2/LE07_173083_20100203 2010-02-03 08:14 92.12     92.10  4.74 73.32 52.10\n",
      "LANDSAT/LE07/C02/T1_L2/LE07_173083_20100219 2010-02-19 08:14 92.01     92.01  4.58 66.61 49.04\n"
     ]
    }
   ],
   "source": [
    "# Note: soure images captured from 2010-01-22 - 2010-02-01\n",
    "gd.Initialize()\n",
    "\n",
    "# create and search the Landsat-7 collection\n",
    "l7_coll = gd.MaskedCollection.from_name('LANDSAT/LE07/C02/T1_L2')\n",
    "l7_coll = l7_coll.search(\n",
    "    '2010-01-01', '2010-02-28', region, cloudless_portion=50\n",
    ")\n",
    "print('Image property descriptions:\\n\\n' + l7_coll.schema_table)\n",
    "print('\\nSearch Results:\\n\\n' + l7_coll.properties_table)\n",
    "\n",
    "# equivalent geedim command line:\n",
    "# !geedim search -c l7-c2-l2 -s 2010-01-01 -e 2010-02-28 -cp 50  -r {src_mosaic_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24a48b5de38495a82d28e85f18a2677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "l7_comp_ref.tif: |                                                                  | 0.00/367M (raw) [  0.0%]…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a composite of L7 search results to fill in SLE gaps\n",
    "l7_comp_im = l7_coll.composite(\n",
    "    gd.CompositeMethod.q_mosaic, date='2010-01-27', \n",
    "    resampling=gd.ResamplingMethod.bilinear,\n",
    ")\n",
    "\n",
    "# get source mosaic CRS for download parameter\n",
    "with rio.open(src_mosaic_path, 'r') as ds:\n",
    "    src_crs = ds.crs.to_wkt()\n",
    "\n",
    "# download\n",
    "l7_comp_im.download(\n",
    "    l7_ref_path, crs=src_crs, scale=30, region=region, scale_offset=True, \n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# equivalent geedim command line:\n",
    "# !geedim composite  -cm q_mosaic --date 2010-01-27 -i LANDSAT/LE07/C02/T1_L2/LE07_173083_20100203 -i LANDSAT/LE07/C02/T1_L2/LE07_173083_20100219 download --crs {src_crs}  --scale 30 -r {src_mosaic_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image property descriptions:\n",
      "\n",
      "ABBREV  NAME               DESCRIPTION\n",
      "------  -----------------  -----------------------------\n",
      "ID      system:id          Earth Engine image id\n",
      "DATE    system:time_start  Image capture date/time (UTC)\n",
      "FILL    FILL_PORTION       Portion of valid pixels (%)\n",
      "\n",
      "Search Results:\n",
      "\n",
      "ID                           DATE             FILL\n",
      "---------------------------- ---------------- ----\n",
      "MODIS/006/MCD43A4/2022_01_27 2022-01-27 00:00  100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8f1ff84e76422584bc8b9648b86f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modis_nbar_ref.tif: |                                                               | 0.00/278k (raw) [  0.0%]…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# search the MODIS NBAR collection\n",
    "modis_coll = gd.MaskedCollection.from_name('MODIS/006/MCD43A4')\n",
    "modis_coll = modis_coll.search(\n",
    "    '2022-01-27', '2022-01-28', region, \n",
    ")\n",
    "print('Image property descriptions:\\n\\n' + modis_coll.schema_table)\n",
    "print('\\nSearch Results:\\n\\n' + modis_coll.properties_table)\n",
    "\n",
    "# download\n",
    "modis_image = gd.MaskedImage.from_id(\n",
    "    'MODIS/006/MCD43A4/2010_01_27', mask=True\n",
    ")\n",
    "modis_image.download(\n",
    "    modis_ref_path, region=region, scale_offset=False, overwrite=True\n",
    ")\n",
    "\n",
    "# equivalent geedim command line:\n",
    "# !geedim search -c modis-nbar -s 2010-01-27 -e 2010-01-28 -r {src_mosaic_path} download -o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction\n",
    "\n",
    "Setup correction parameters and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homonim import Model, ProcCrs, utils\n",
    "if False:\n",
    "    # fuse with Landsat-7 and compare with MODIS NBAR\n",
    "    ref_bands = [3, 2, 1, 4]\n",
    "    cmp_ref_bands = [1, 4, 3, 2]\n",
    "    ref_path = l7_ref_path\n",
    "    cmp_ref_path = modis_ref_path\n",
    "    model = Model.gain\n",
    "    kernel_shape = (5, 5)\n",
    "    corr_root = Path(\n",
    "        'V:/Data/HomonimEgs/NGI_3321BD-3322AC_2010_LittleKaroo/Corrected/'\n",
    "        'GroundTruthSubset/L7/'\n",
    "    )\n",
    "    ref_sc_off = [.2, 0]\n",
    "    out_profile = None\n",
    "else:\n",
    "    # fuse with MODIS NBAR and compare with Landsat-7\n",
    "    ref_bands = [1, 4, 3, 2]\n",
    "    cmp_ref_bands = [3, 2, 1, 4]\n",
    "    ref_path = modis_ref_path\n",
    "    cmp_ref_path = l7_ref_path\n",
    "    model = Model.gain\n",
    "    kernel_shape = (1, 1)\n",
    "    corr_root = Path(\n",
    "        'V:/Data/HomonimEgs/NGI_3321BD-3322AC_2010_LittleKaroo/Corrected/'\n",
    "        'GroundTruthSubset/Modis/'\n",
    "    )\n",
    "    ref_sc_off = [1.7e3, .1]\n",
    "    out_profile = dict(dtype='uint16', nodata=0)\n",
    "\n",
    "post_fix = utils.create_out_postfix(\n",
    "    ProcCrs.ref, model=model, kernel_shape=kernel_shape, driver='GTiff'\n",
    ")\n",
    "corr_paths = [\n",
    "    corr_root.joinpath(src_path.stem + post_fix)\n",
    "    for src_path in src_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuse aerial images with reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321b_3172_12_0415_rgbn_CMP_FUSE_cREF_mGAIN_k1_1.tif:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7daf828de9f450c9a53ba758a6ecd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321b_3172_12_0419_rgbn_CMP_FUSE_cREF_mGAIN_k1_1.tif:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b3a2aac387424d8c2bae91b330c0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321D_319_04_0121_RGBN_CMP_FUSE_cREF_mGAIN_k1_1.tif:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6884958c6924d2594521187d4e64577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3322c_322_02_0056_rgbn_CMP_FUSE_cREF_mGAIN_k1_1.tif:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4948b9e80e204b96a80650045967eecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from homonim import RasterFuse\n",
    "\n",
    "for src_path, corr_path in zip(src_paths, corr_paths):\n",
    "    with RasterFuse(src_path, ref_path, ref_bands=ref_bands) as raster_fuse:\n",
    "        print(f'{corr_path.name}:')\n",
    "        raster_fuse.process(\n",
    "            corr_path, model, kernel_shape, \n",
    "            block_config=dict(threads=4, max_block_mem=512), \n",
    "            out_profile=out_profile, \n",
    "            model_config=dict(mask_partial=False), \n",
    "            overwrite=True, \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a VRT mosaic of the corrected images to use in the visualisation and evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "# strictly, one should avoid using GDAL and rasterio together, but it doesn't\n",
    "# create conflicts here\n",
    "corr_mosaic_path = corr_root.joinpath(\n",
    "    f'NGI_3321BD-3322AC_2010_LittleKaroo_GroundTruthSubset{post_fix[:-4]}.vrt'\n",
    ")\n",
    "ds = gdal.BuildVRT(\n",
    "    str(corr_mosaic_path.absolute()), [str(cp) for cp in corr_paths]\n",
    ")\n",
    "ds.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise\n",
    "\n",
    "Display matching extents of the source, reference and corrected images, in the reference CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.vrt import WarpedVRT\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# get source crs\n",
    "with rio.open(src_mosaic_path, 'r') as ds:\n",
    "    src_crs = ds.crs\n",
    "\n",
    "fig, axes = pyplot.subplots(\n",
    "    3, 1, sharex=True, sharey=True, tight_layout=True, figsize=(10, 15), \n",
    "    dpi=100\n",
    ")\n",
    "\n",
    "# loop over src, ref and corrected images and corrsonding parameters\n",
    "for im_file, ds_fact, indexes, sc_off, ax, label in zip(\n",
    "    [src_mosaic_path, ref_path, corr_mosaic_path],\n",
    "    [16, 1, 16],                           # downsample factor\n",
    "    [None, ref_bands, None],               # band indices\n",
    "    [None, ref_sc_off, ref_sc_off],        # colour scale & offset\n",
    "    axes,\n",
    "    ['Source', 'Reference', 'Corrected'], \n",
    "):\n",
    "    # read image\n",
    "    with (rio.open(im_file, 'r')) as ds:\n",
    "        if ds.crs != src_crs:\n",
    "            # re-project to source CRS\n",
    "            ds = WarpedVRT(ds, crs=src_crs, resampling=Resampling.bilinear)\n",
    "        with ds:\n",
    "            # read and downsample image\n",
    "            ds_shape = tuple(np.round(np.array(ds.shape) / ds_fact).astype('int'))\n",
    "            transform = ds.transform * rio.Affine.scale(ds_fact)\n",
    "\n",
    "            array = ds.read(indexes=indexes, out_dtype='float32', out_shape=ds_shape)\n",
    "\n",
    "    # change nodata value to nan\n",
    "    mask = np.any((array == ds.nodata) | np.isnan(array), axis=(0))\n",
    "    array[:, mask] = np.nan\n",
    "\n",
    "    # contrast stretching for display\n",
    "    if sc_off is not None:\n",
    "        # scale and offset pixel values\n",
    "        array = np.clip((array / sc_off[0]) - sc_off[1], 0, 1)\n",
    "    else:\n",
    "        # 'normalise' image 2%-98% -> 0-1\n",
    "        for bi in range(array.shape[0]):\n",
    "            array[bi] -= np.nanpercentile(array[bi], 1)\n",
    "            array[bi] /= np.nanpercentile(array[bi], 99)\n",
    "            array[bi] = np.clip(array[bi], 0, 1)\n",
    "\n",
    "    # display\n",
    "    ax = show(array[0:3], transform=transform, interpolation='nearest', ax=ax)\n",
    "    ax.set_title(label, fontweight='bold')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display close-up of ground truth polygons on corrected image background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# read ground truth polygons into geo dataframe\n",
    "groundtruth_path = ref_root.joinpath('LittleKaroo3ClassGroundTruth.gpkg')\n",
    "class_label_gdf = gpd.GeoDataFrame.from_file(groundtruth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import features, windows\n",
    "from homonim import utils\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# ground truth subset of interest\n",
    "idx = None\n",
    "for i in [80, 81, 83, 84]:\n",
    "    if idx is not None:\n",
    "        idx |= (class_label_gdf.index == i)\n",
    "    else:\n",
    "        idx = (class_label_gdf.index == i)\n",
    "sub_class_label_gdf = class_label_gdf.loc[idx]\n",
    "\n",
    "fig, ax = pyplot.subplots(1, 1, tight_layout=True, figsize=(5, 4), dpi=92)\n",
    "indexes = (1, 2, 3)\n",
    "\n",
    "# read, normalise and display corrected mosaic\n",
    "with rio.open(corr_mosaic_path, 'r') as corr_ds:\n",
    "    # find window containing ground truth subset\n",
    "    sub_class_label_gdf = sub_class_label_gdf.to_crs(corr_ds.crs.to_wkt())\n",
    "    bounds = sub_class_label_gdf.total_bounds\n",
    "    win = corr_ds.window(*bounds)\n",
    "    win = utils.expand_window_to_grid(win, (40, 40))\n",
    "    transform = corr_ds.window_transform(win)\n",
    "    \n",
    "    # read image corresponding to window\n",
    "    array = corr_ds.read(\n",
    "        indexes=indexes, out_dtype='float32', window=win,\n",
    "    )\n",
    "\n",
    "    # change nodata value to nan\n",
    "    mask = np.any(array == corr_ds.nodata, axis=(0))\n",
    "    array[:, mask] = np.nan\n",
    "    \n",
    "    # 'normalise' image 2%-98% -> 0-1\n",
    "    for bi in range(len(indexes)):\n",
    "        array[bi] -= np.nanpercentile(array[bi], 1)\n",
    "        array[bi] /= np.nanpercentile(array[bi], 95)\n",
    "        array[bi] = np.clip(array[bi], 0, 1)\n",
    "\n",
    "    ax = show(array, transform=transform, interpolation='bilinear', ax=ax)\n",
    "\n",
    "cmap = ListedColormap([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "ax = sub_class_label_gdf.plot(\n",
    "        'Class', kind='geo', ax=ax, linewidth=1.5, facecolor='none',\n",
    "        cmap=cmap, legend=True, alpha=0.6, legend_kwds=dict(loc='lower right')\n",
    "    )\n",
    "_ = ax.axis('tight')\n",
    "_ = ax.axis('off')\n",
    "\n",
    "fig.savefig('../case_studies/classification-groundtruth_polygons.jpg', dpi=92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Compare corrected images with a second surface reflectance reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homonim import RasterCompare\n",
    "\n",
    "print(RasterCompare.schema_table())\n",
    "\n",
    "# loop over the source and corrected image files\n",
    "for im_path, im_label in zip(\n",
    "    [src_mosaic_path, corr_mosaic_path],\n",
    "    ['Source', 'Corrected'],\n",
    "):\n",
    "    with RasterCompare(\n",
    "        im_path, cmp_ref_path, ref_bands=cmp_ref_bands,\n",
    "    ) as compare:\n",
    "        # print a table of comparison statistics (the typical way of using \n",
    "        # RasterCompare)\n",
    "        print(f'{im_label}:')\n",
    "        stats_dict = compare.process()\n",
    "        print(f'{im_label} comparison:\\n\\n' + compare.stats_table(stats_dict))\n",
    "\n",
    "    # equivalent homonim command line:\n",
    "    # !homonim compare {im_path} {cmp_ref_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Compare classification performance before and after correction with `homonim`.  \n",
    "\n",
    "### Feature extraction\n",
    "\n",
    "Extract band pixel values (features) for ground truth polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import features\n",
    "from rasterio import windows\n",
    "\n",
    "def extract_feats(\n",
    "    ds: rio.DatasetReader, labelled_plot_gdf: gpd.GeoDataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Extract pixel features from image for given plots. \"\"\"\n",
    "    px_feats_dict = dict(plot_id=[], label=[])\n",
    "\n",
    "    # loop over ground truth plots\n",
    "    for plot_id, plot in tqdm(\n",
    "        labelled_plot_gdf.to_crs(ds.crs).iterrows(), \n",
    "        total=labelled_plot_gdf.shape[0], desc=im_path.name, \n",
    "        dynamic_ncols=True,\n",
    "    ):\n",
    "        if plot.Class is None:\n",
    "            continue\n",
    "\n",
    "        # find a rasterio window that contains the plot polygon\n",
    "        bounds = features.bounds(plot.geometry)\n",
    "        win = windows.from_bounds(*bounds, transform=ds.transform)\n",
    "        win = utils.expand_window_to_grid(win)\n",
    "\n",
    "        # read the image array corresponding to the plot window\n",
    "        array = ds.read(out_dtype='float64', window=win)\n",
    "\n",
    "        # find a mask for pixels contained inside the plot polygon\n",
    "        win_transform = ds.window_transform(win)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')     # ignore shapely deprecation warning            \n",
    "            mask = ~features.geometry_mask(\n",
    "                plot.geometry, (win.height, win.width), transform=win_transform\n",
    "            )\n",
    "\n",
    "        def app_listval(d: dict, k: str, val: list):\n",
    "            \"\"\" Append a list to a dictionary value. \"\"\"\n",
    "            if k in d:\n",
    "                d[k] += val\n",
    "            else:\n",
    "                d[k] = val\n",
    "            return d\n",
    "\n",
    "        # extract band values for pixels inside polygon\n",
    "        for bi, fname in zip([0, 1, 2, 3], ['Red', 'Green', 'Blue', 'NIR']):\n",
    "            feat = array[bi]\n",
    "            featv = list(feat[mask])\n",
    "            px_feats_dict = app_listval(px_feats_dict, fname, featv)\n",
    "\n",
    "        # copy plot id and class label for these pixels\n",
    "        px_feats_dict['plot_id'] += [plot_id] * len(featv)\n",
    "        px_feats_dict['label'] += [plot.Class] * len(featv)\n",
    "    \n",
    "    df = pd.DataFrame(px_feats_dict)\n",
    "    return df\n",
    "\n",
    "# loop over source and corrected image mosaics\n",
    "feats = []\n",
    "for im_path in [src_mosaic_path, corr_mosaic_path]:    \n",
    "    # extract and store features\n",
    "    with rio.open(im_path, 'r') as ds:\n",
    "        feats_gdf = extract_feats(ds, class_label_gdf)\n",
    "    feats_gdf['label_i'] = pd.Categorical(feats_gdf['label']).codes\n",
    "    feats.append(feats_gdf)\n",
    "feats_dict = dict(zip(['Source', 'Corrected'], feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature visualisation\n",
    "\n",
    "Visualise scatter of source and corrected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import cm\n",
    "\n",
    "def class_kde(\n",
    "    df: pd.DataFrame, x=None, y=None, label_col='label', ax=None\n",
    "):\n",
    "    \"\"\" Feature KDE plot, with class colour coding. \"\"\"\n",
    "    colours = dict(zip(\n",
    "        pd.Categorical(feats_gdf[label_col]).categories, \n",
    "        [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "    ))\n",
    "    \n",
    "    def get_ptile_range(v, n=200):\n",
    "        \"\"\" Return a percentile linear range of the given vector. \"\"\"\n",
    "        return np.linspace(\n",
    "            np.percentile(v, .25), np.percentile(v, 98), n\n",
    "        )\n",
    "    \n",
    "    # create KDE grid\n",
    "    xrange = get_ptile_range(df[x])\n",
    "    yrange = get_ptile_range(df[y])\n",
    "    xx, yy = np.meshgrid(xrange, yrange)  \n",
    "    pos = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    kdes = []\n",
    "    \n",
    "    # create per-class KDEs\n",
    "    for label, group in df.groupby(label_col):\n",
    "        val = np.vstack([group[x], group[y]])\n",
    "        kernel = stats.gaussian_kde(val)\n",
    "        kde = np.reshape(kernel(pos).T, xx.shape)\n",
    "        kde /= kde.max()\n",
    "        kdes.append(kde)\n",
    "        # cset = ax.contour(\n",
    "        #     xx, yy, kde, colors=colours[label], alpha=0.4, \n",
    "        # )\n",
    "        # dummy plot for the legend\n",
    "        ax.plot(xx[0], yy[0], 'o', ms=10, alpha=0, label=label, color=colours[label])\n",
    "    \n",
    "    # display RGB image of KDEs\n",
    "    im = np.dstack(kdes)\n",
    "    ax.imshow(\n",
    "        im, extent=[xrange[0], xrange[-1], yrange[0], yrange[-1]], \n",
    "        interpolation='bilinear', origin='lower'\n",
    "    )\n",
    "    ax.axis('auto')\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    for lh in ax.legend(loc='upper left').legendHandles: \n",
    "        lh.set_alpha(.6)\n",
    "        lh.set_markeredgecolor('none')\n",
    "\n",
    "\n",
    "fig, axes = pyplot.subplots(\n",
    "    1, 2, tight_layout=True, figsize=(8, 4), dpi=92, \n",
    ")\n",
    "\n",
    "for feats_df, ax, label in zip(\n",
    "    feats_dict.values(), axes, feats_dict.keys(),\n",
    "):\n",
    "    class_kde(feats_df.iloc[::10], 'NIR', 'Blue', ax=ax)\n",
    "    ax.set_title(label, fontweight='bold')\n",
    "\n",
    "fig.savefig('../case_studies/classification-spectral_kde.jpg', dpi=92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy\n",
    "\n",
    "Evaluate the accuracy of a naive Bayes classifier on the source and corrected features.  First resample the data so that there are equal number of samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample classes to have == number of samples\n",
    "# from sklearn.utils import resample\n",
    "# print(feats_dict['Source'].groupby('label').size())\n",
    "# nc = feats_dict['Source'].groupby('label').size()\n",
    "# mnc = nc.min()\n",
    "\n",
    "# feats_dict_rs = {}\n",
    "# for feat_label, feats_df in feats_dict.items():\n",
    "#     rs_list = []\n",
    "#     for group_label, group in feats_df.groupby('label'):\n",
    "#         if group.shape[0] > mnc:\n",
    "#             group_rs = group.sample(n=mnc, replace=False)\n",
    "#         else:\n",
    "#             group_rs = group\n",
    "#         rs_list.append(group_rs)\n",
    "#     feats_dict_rs[feat_label] = pd.concat(rs_list)\n",
    "#     print(feats_dict_rs[feat_label].groupby('label').size())\n",
    "# feats_dict = feats_dict_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "\n",
    "def train_test(\n",
    "    X: np.ndarray, y: np.ndarray, groups: np.ndarray = None, n_splits=10,\n",
    "    clf = GaussianNB()\n",
    ") -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Train and test a classifier on the given features and labels using \n",
    "    cross-validation. \n",
    "    \"\"\"\n",
    "    if groups is not None:\n",
    "        tr_ts_indices = StratifiedGroupKFold(n_splits=n_splits).split(\n",
    "            X, y, groups\n",
    "        )\n",
    "    else:\n",
    "        tr_ts_indices = StratifiedKFold(n_splits=n_splits).split(X, y)\n",
    "    \n",
    "    ypred = np.zeros_like(y)\n",
    "    yscore = np.zeros((len(y), 3))\n",
    "    for train, test in tqdm(\n",
    "        tr_ts_indices, dynamic_ncols=True, total=n_splits\n",
    "    ):\n",
    "        Xtr = X[train, :]\n",
    "        ytr = y[train]\n",
    "        Xts = X[test, :]\n",
    "        yts = y[test]\n",
    "        clf.fit(Xtr, ytr)\n",
    "        ypred[test] = clf.predict(Xts)\n",
    "        yscore[test] = clf.predict_proba(Xts)\n",
    "    return ypred, yscore\n",
    "\n",
    "# loop over source and corrected features\n",
    "label_col = 'label'\n",
    "for df_label, df in feats_dict.items():\n",
    "        # raw spectral band feature matrix\n",
    "    X = df[['Red', 'Green', 'Blue', 'NIR']].to_numpy()\n",
    "    # convert string class labels to int codes\n",
    "    y = pd.Categorical(df[label_col]).codes\n",
    "    \n",
    "    class_labels = pd.Categorical(df[label_col]).categories\n",
    "    # equal prior weighting for classifier\n",
    "    priors = np.ones(len(class_labels)) / len(class_labels)\n",
    "    if False:\n",
    "        # pipeline of scaler, PCA & naive bayes\n",
    "        scaler = StandardScaler()\n",
    "        pca = PCA(n_components=2)\n",
    "        nb = GaussianNB(priors=priors)\n",
    "        clf = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"clf\", nb)])\n",
    "    else:\n",
    "        clf = GaussianNB(priors=priors)\n",
    "\n",
    "    ypred, yscore = train_test(\n",
    "        X, y, groups=df.plot_id.to_numpy(), clf=clf, n_splits=10\n",
    "    )\n",
    "    # yscore = 1 - np.sum(yscore[:, [0, 2]], axis=1)\n",
    "\n",
    "    # find and print accuracy scores\n",
    "    index = [f'**{cl}**' for cl in class_labels]\n",
    "    cm = confusion_matrix(y, ypred)\n",
    "    cm_norm = cm / cm.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    cm_df = pd.DataFrame(\n",
    "        data=cm, columns=class_labels, index=index\n",
    "    )\n",
    "    cm_df['Total'] = cm_df.sum(axis=1)\n",
    "    cm_df.loc['**Total**'] = cm_df.sum(axis=0)\n",
    "\n",
    "    cm_norm_df = pd.DataFrame(\n",
    "        data=cm_norm, columns=class_labels, index=index\n",
    "    )\n",
    "\n",
    "    print(f'{df_label}:')\n",
    "    print(f'Confusion matrix:')\n",
    "    print(tabulate(\n",
    "        cm_df, headers=cm_df.columns, tablefmt='grid', \n",
    "    ))\n",
    "    print(f'\\nConfusion matrix (normalised):')\n",
    "    print(tabulate(\n",
    "        cm_norm_df, headers=cm_norm_df.columns, tablefmt='grid', floatfmt='.2f'\n",
    "    ))\n",
    "    print(f'\\nAccuracy: {100*np.sum(np.diag(cm))/np.sum(cm[:]):.2f}')\n",
    "    print(f'Accuracy (normalised): {100*np.mean(np.diag(cm_norm)):.2f}')\n",
    "    \n",
    "    auc = roc_auc_score(y, yscore, multi_class='ovo')\n",
    "    print(f'AUC: {auc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5cb0e16c618ce62d5962a2f34302e2d299d083ff8e8afee2c1c64a34662e4b4"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08a7dfb09fe243f7a21de85b136f283a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "0c09dc8994d746788e57d64caa72b882": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0d919aca941048fc95299497563fd547": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_682ca5e81cff405d82886b2ef121daca",
       "style": "IPY_MODEL_7a10078b89f04ffe87e4b13a139caa09",
       "value": " 367M/367M (raw) [100.0%] in 00:51 (eta: 00:00)"
      }
     },
     "0f8f1ff84e76422584bc8b9648b86f24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bc8dc28ff71e4b979a8245db89371821",
        "IPY_MODEL_20c7d15761ac48a5a9d0069e7afd11a1",
        "IPY_MODEL_37c3a839d87b4a5c944ef2ffc381e16a"
       ],
       "layout": "IPY_MODEL_08a7dfb09fe243f7a21de85b136f283a"
      }
     },
     "127c19160ab6438eb3ef6ea36f24e1c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1c88e8c8373d419fbb41abf9560a47c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_534f62f3ef6f4d38aa04fa54edec1bdb",
       "style": "IPY_MODEL_127c19160ab6438eb3ef6ea36f24e1c5",
       "value": "100%"
      }
     },
     "20c7d15761ac48a5a9d0069e7afd11a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_f6d429154dfa4a8ab1be81164de20a5e",
       "max": 278160,
       "style": "IPY_MODEL_992dd93718f744a6808495081568e562",
       "value": 278160
      }
     },
     "24214b7b94be4ac498d1e7583f9677fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "27314f661c27478aa48ca84294a90b0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "27acb7e902d248c783e51b95854f1895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_abd0dd2290524e4a89835e0baa9e384b",
       "style": "IPY_MODEL_7f4327edfcba4fd59e07b17076d2b6e5",
       "value": "l7_comp_ref.tif: "
      }
     },
     "2a3b547f89584a30a560dda9121c0c6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "37c3a839d87b4a5c944ef2ffc381e16a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5447200cd76b44fc80257a4cc6a80bee",
       "style": "IPY_MODEL_eb3025fd24c84e6aac51c1460fc8ec07",
       "value": " 278k/278k (raw) [100.0%] in 00:04 (eta: 00:00)"
      }
     },
     "3cf14407a42e45c794e52370ad2df956": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3fbf8fee39274fea8f1073b9603421d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_689d51f4b11b412aba732f019bb22f01",
       "style": "IPY_MODEL_839e8230062043b28549e40e0dfae1f3",
       "value": " 62%"
      }
     },
     "44187d63c29743d8866eeda50d282562": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b9829870c4944fa0b8429758ca8bee25",
       "max": 4,
       "style": "IPY_MODEL_c06dc45755414225858f4d643e4b341d",
       "value": 4
      }
     },
     "4948b9e80e204b96a80650045967eecd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3fbf8fee39274fea8f1073b9603421d7",
        "IPY_MODEL_99d62876ad254a0ea1defbab9d63c0a9",
        "IPY_MODEL_f2ec9ec3b049451ab8db5ab3091f78f0"
       ],
       "layout": "IPY_MODEL_2a3b547f89584a30a560dda9121c0c6f"
      }
     },
     "4b8bdc9b157848adaedb0c791d98285f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c027a1f6e1064938bfc34f09200acf99",
       "style": "IPY_MODEL_dfe4bd95e9d14ea8b7b28c8ebd555380",
       "value": "100%"
      }
     },
     "51ab3d5fdb9b42d897e4242e4e2ff448": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "534f62f3ef6f4d38aa04fa54edec1bdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5447200cd76b44fc80257a4cc6a80bee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "573a1c066de4499fb78fc3fa2bc256f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_5786bbd6f5c74561854693a5b1d650bf",
       "max": 367222080,
       "style": "IPY_MODEL_95acaf1ea32341478092c8f086d85ca1",
       "value": 367222080
      }
     },
     "5786bbd6f5c74561854693a5b1d650bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "5a10d8e84428459b92fd32453438e3fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5c5c131460f04984b31c315bd80f7446": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d8472b2c5d75492ab8c0ca605c6f7042",
       "style": "IPY_MODEL_3cf14407a42e45c794e52370ad2df956",
       "value": "4/4 blocks [01:28&lt;00:00]"
      }
     },
     "682ca5e81cff405d82886b2ef121daca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "689d51f4b11b412aba732f019bb22f01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7a10078b89f04ffe87e4b13a139caa09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7bab3b7fd05c4f49869b028bce82c4cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "7f4327edfcba4fd59e07b17076d2b6e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "839e8230062043b28549e40e0dfae1f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "849329d151fc48dd8ab9a4b986a5b803": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c54fba9151f46e982e65e3843af02c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5a10d8e84428459b92fd32453438e3fb",
       "style": "IPY_MODEL_d6fbe3994d6d4eb18941fdb66df6ef60",
       "value": "100%"
      }
     },
     "94fca7a0c2054a71aeeb696c3d48f0bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "9540272893b44e579e667b8a7c6f1f92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "95acaf1ea32341478092c8f086d85ca1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "992dd93718f744a6808495081568e562": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "99d62876ad254a0ea1defbab9d63c0a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_b9547413378642baadcfe503e7823a27",
       "max": 8,
       "style": "IPY_MODEL_27314f661c27478aa48ca84294a90b0a",
       "value": 5
      }
     },
     "a59e149b0e494e14a541f5d27c21678c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_be38f58bff014115b7cdd3a44654f53c",
       "max": 8,
       "style": "IPY_MODEL_dfc13abd46324f1e9cd25bd54c5abfb8",
       "value": 8
      }
     },
     "a79041ebf6d244e3936ca93a7badb3b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "abd0dd2290524e4a89835e0baa9e384b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b6884958c6924d2594521187d4e64577": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1c88e8c8373d419fbb41abf9560a47c2",
        "IPY_MODEL_a59e149b0e494e14a541f5d27c21678c",
        "IPY_MODEL_ba639cc6f8db4542baf2fc1e35d90859"
       ],
       "layout": "IPY_MODEL_d83b8fba845541d3879c9c16b926bda0"
      }
     },
     "b9547413378642baadcfe503e7823a27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "b9829870c4944fa0b8429758ca8bee25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "ba639cc6f8db4542baf2fc1e35d90859": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_849329d151fc48dd8ab9a4b986a5b803",
       "style": "IPY_MODEL_c485fb7b3cd64b91bb742e898bb90b64",
       "value": "8/8 blocks [01:12&lt;00:00]"
      }
     },
     "bbe752e784434430a6f54e345f8f9368": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bc8dc28ff71e4b979a8245db89371821": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_24214b7b94be4ac498d1e7583f9677fa",
       "style": "IPY_MODEL_9540272893b44e579e667b8a7c6f1f92",
       "value": "modis_nbar_ref.tif: "
      }
     },
     "be38f58bff014115b7cdd3a44654f53c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "c027a1f6e1064938bfc34f09200acf99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c06dc45755414225858f4d643e4b341d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c132ac9882e74df18537acdf326d5b73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_51ab3d5fdb9b42d897e4242e4e2ff448",
       "style": "IPY_MODEL_eab0884f2f1a447791372581ffb351a5",
       "value": "4/4 blocks [01:23&lt;00:00]"
      }
     },
     "c485fb7b3cd64b91bb742e898bb90b64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c88eb4dfe35d464ea5a5063e1fd0d45c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "d6fbe3994d6d4eb18941fdb66df6ef60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d797b8a901f7472781e085b5255aebdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d7daf828de9f450c9a53ba758a6ecd89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4b8bdc9b157848adaedb0c791d98285f",
        "IPY_MODEL_d8526d70fe094307816194ecd76af085",
        "IPY_MODEL_5c5c131460f04984b31c315bd80f7446"
       ],
       "layout": "IPY_MODEL_94fca7a0c2054a71aeeb696c3d48f0bd"
      }
     },
     "d83b8fba845541d3879c9c16b926bda0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "d8472b2c5d75492ab8c0ca605c6f7042": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d8526d70fe094307816194ecd76af085": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a79041ebf6d244e3936ca93a7badb3b2",
       "max": 4,
       "style": "IPY_MODEL_d797b8a901f7472781e085b5255aebdd",
       "value": 4
      }
     },
     "dfc13abd46324f1e9cd25bd54c5abfb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dfe4bd95e9d14ea8b7b28c8ebd555380": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e24a48b5de38495a82d28e85f18a2677": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_27acb7e902d248c783e51b95854f1895",
        "IPY_MODEL_573a1c066de4499fb78fc3fa2bc256f0",
        "IPY_MODEL_0d919aca941048fc95299497563fd547"
       ],
       "layout": "IPY_MODEL_c88eb4dfe35d464ea5a5063e1fd0d45c"
      }
     },
     "eab0884f2f1a447791372581ffb351a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "eb3025fd24c84e6aac51c1460fc8ec07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f2ec9ec3b049451ab8db5ab3091f78f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0c09dc8994d746788e57d64caa72b882",
       "style": "IPY_MODEL_bbe752e784434430a6f54e345f8f9368",
       "value": "5/8 blocks [00:50&lt;00:21]"
      }
     },
     "f3b3a2aac387424d8c2bae91b330c0ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8c54fba9151f46e982e65e3843af02c0",
        "IPY_MODEL_44187d63c29743d8866eeda50d282562",
        "IPY_MODEL_c132ac9882e74df18537acdf326d5b73"
       ],
       "layout": "IPY_MODEL_7bab3b7fd05c4f49869b028bce82c4cd"
      }
     },
     "f6d429154dfa4a8ab1be81164de20a5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
